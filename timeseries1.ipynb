{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from transformer_common import TransformerConfig, TransformerRunner\n",
    "import matplotlib.pyplot as plt\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# https://www.kaggle.com/datasets/footballjoe789/us-stock-dataset/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b1eb6586d55caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor loaded successfully: stock_history_data.pt, len=6262\n",
      "Execution time: 0.0164 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3l/9k1jwyrx3nnfw4578m3876ww0000gn/T/ipykernel_11896/3224761579.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(input_tensor_file)\n"
     ]
    }
   ],
   "source": [
    "from transformer_common import TimeseriesDataloader\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "input_tensor_file = 'stock_history_data.pt'\n",
    "\n",
    "if os.path.exists(input_tensor_file):\n",
    "    data = torch.load(input_tensor_file)\n",
    "    ln = len(data)\n",
    "    print(f\"Tensor loaded successfully: {input_tensor_file}, len={ln}\")\n",
    "else:\n",
    "    directory_path = './us-stock-dataset/Data/StockHistory'\n",
    "\n",
    "    all_files = [os.path.splitext(f)[0] for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "    \n",
    "    stocks_to_load = set(all_files)\n",
    "    dataloader = TimeseriesDataloader(directory_path, stocks_to_load, add_diff=False)\n",
    "    \n",
    "    data=dataloader.get_data().transpose(0, 1).cuda()\n",
    "\n",
    "    torch.save(data, input_tensor_file)\n",
    "\n",
    "    print(f\"Tensor saved successfully: {input_tensor_file}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b95620e62a0534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6262, 5283])\n",
      "shape after filtering torch.Size([4735, 5283])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def filter_invalid_stocks(tensor):\n",
    "    \"\"\"\n",
    "    Filters out stocks (rows) that have all zero values, all NaN values, or no price change (constant values) across their time series.\n",
    "    \n",
    "    Parameters:\n",
    "    - tensor (torch.Tensor): Time-series data with shape [num_stocks, time_steps].\n",
    "\n",
    "    Returns:\n",
    "    - filtered_tensor (torch.Tensor): Tensor with invalid stocks removed.\n",
    "    \"\"\"\n",
    "    # Replace NaNs with zeros in the tensor\n",
    "    tensor = torch.nan_to_num(tensor, nan=0.0)\n",
    "\n",
    "    # Identify stocks that have all zeroes or all NaNs (now converted to zeros)\n",
    "    non_zero_stocks = torch.any(tensor != 0, dim=1)  # Only keep rows (stocks) that have non-zero values\n",
    "\n",
    "    # Identify stocks where there is no price change (i.e., variance is zero)\n",
    "    non_constant_stocks = torch.var(tensor, dim=1) != 0  # Keep stocks with non-zero variance\n",
    "\n",
    "    # Combine both conditions (stocks with non-zero values and non-constant prices)\n",
    "    valid_stocks = non_zero_stocks & non_constant_stocks\n",
    "\n",
    "    # Filter out invalid stocks\n",
    "    filtered_tensor = tensor[valid_stocks]\n",
    "\n",
    "    return filtered_tensor\n",
    "print(data.shape)\n",
    "\n",
    "data = filter_invalid_stocks(data)\n",
    "print(f\"shape after filtering {data.shape}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80793d80bef7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_timeseries(tensor, num_charts=5):\n",
    "    \"\"\"\n",
    "    Plots time-series data for multiple stocks from the tensor in a grid layout with 5 charts per row.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor (torch.Tensor): Time-series data with shape [num_stocks, time_steps].\n",
    "    - num_charts (int): The number of charts to plot. Each chart corresponds to one stock.\n",
    "    \"\"\"\n",
    "    # Check the number of stocks\n",
    "    num_stocks, num_time_steps = tensor.shape\n",
    "\n",
    "    # Ensure num_charts doesn't exceed the number of available stocks\n",
    "    num_charts = min(num_charts, num_stocks)\n",
    "\n",
    "    # Calculate the number of rows required for the grid\n",
    "    rows = (num_charts + 4) // 5  # This ensures that we have a full row for the remaining charts\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(rows, 5, figsize=(15, 3 * rows))\n",
    "    \n",
    "    # Flatten axes array to easily index through them\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot each stock's time-series data\n",
    "    for i in range(num_charts):\n",
    "        ax = axes[i]\n",
    "        ax.plot(tensor[i].cpu().numpy())  # Move tensor to CPU and convert to numpy for plotting\n",
    "        ax.set_title(f\"Stock {i+1} - Time Series\")\n",
    "        ax.set_xlabel(\"Time Steps\")\n",
    "        ax.set_ylabel(\"Stock Value\")\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_charts, len(axes)):\n",
    "        axes[i].axis('off')  # Hide the empty subplots\n",
    "\n",
    "    plt.tight_layout()  # Adjust the layout to avoid overlap\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plotting the first 5 stocks' time-series data\n",
    "plot_timeseries(data, num_charts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990c051-7e80-403a-a004-63fd0f57bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.diff(data, dim=1)\n",
    "\n",
    "\n",
    "def scale_timeseries_data(data, dim=0):\n",
    "\n",
    "    # Replace NaNs with zeros\n",
    "    data = torch.nan_to_num(data, nan=0.0)\n",
    "\n",
    "    # Standardize along the specified dimension\n",
    "    # Calculate mean and std along the specified dimension\n",
    "    mean = data.mean(dim=dim, keepdim=True)\n",
    "    std = data.std(dim=dim, keepdim=True)\n",
    "\n",
    "    # Avoid division by zero for dimensions with zero std\n",
    "    std[std == 0] = 1\n",
    "\n",
    "    # Scale the data (standardization)\n",
    "    scaled_data = (data - mean) / std\n",
    "\n",
    "    return scaled_data\n",
    "    \n",
    "data = scale_timeseries_data(data, dim=0)  # Scale each stock independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c276157-d695-4309-8e37-3f77d1650745",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data.transpose(0, 1)\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd1e81-8ceb-4f8a-86f3-f4f65b0bb559",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_channels=4735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cde7ed-5fb2-4ac3-9bab-69307f8c8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data[0:num_of_channels].transpose(0, 1)\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948ca1f-5119-4db1-aae2-9fd578401e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ConvKarpathyTransformerModel\n",
    "\n",
    "config = TransformerConfig(\n",
    "    # precision=torch.bfloat16,\n",
    "    precision=torch.float32,\n",
    "    batch_size=128,\n",
    "    block_size=16,\n",
    "    causal=True,\n",
    "    input_embed=num_of_channels,\n",
    "    n_embed=128,\n",
    "    output_embed=num_of_channels,\n",
    "    n_head=8,\n",
    "    n_layer=16,\n",
    "    learning_rate=1e-4\n",
    ")\n",
    "config.eval_interval=50\n",
    "\n",
    "trainer1 = TransformerRunner(config, ConvKarpathyTransformerModel(config), training_data, training_data)\n",
    "\n",
    "# if not trainer1.load_model(\"abc\"):\n",
    "trainer1.train_iterate_n(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62ab896-478a-4ab0-a501-9fc9a2f20a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a2ea78-86d4-4eaf-bb9a-98ac5f8c7281",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reverse normalization function\n",
    "def inverse_normalize(tensor, original_data,dim=0):\n",
    "    mean = original_data.mean(dim=dim, keepdim=True)\n",
    "    std = original_data.std(dim=dim, keepdim=True)\n",
    "    return tensor * std + mean\n",
    "\n",
    "# Reverse differencing function\n",
    "def inverse_difference(predictions, last_known_value):\n",
    "    restored_values = [last_known_value]  # Start with the last known value\n",
    "    for diff in predictions:\n",
    "        restored_values.append(restored_values[-1] + diff)\n",
    "    return restored_values[1:]  # Skip the initial value\n",
    "\n",
    "# Generate predictions for 100 future days for 20 stocks\n",
    "num_stocks = 20  # The number of stocks you want to predict\n",
    "days_to_predict = 5  # Predict for 100 future days\n",
    "predictions = []\n",
    "\n",
    "context = training_data[-config.block_size:, :].unsqueeze(0)  # Use last block_size days as context\n",
    "print(f\"context={context.shape}\")\n",
    "\n",
    "\n",
    "prediction_diff = trainer1.generate(context, max_new_tokens=days_to_predict).transpose(0, 2).detach()\n",
    "print(prediction_diff.shape)\n",
    "\n",
    "# # Inverse normalization\n",
    "# prediction_diff = inverse_normalize(prediction_diff, training_data)\n",
    "\n",
    "# # Inverse differencing using last value from original data\n",
    "# last_known_value = training_data[:, -1]\n",
    "# prediction = inverse_difference(prediction_diff.squeeze(), last_known_value)\n",
    "# predictions.append(prediction)\n",
    "\n",
    "# # Plot the predictions for each stock\n",
    "# fig, axes = plt.subplots(4, 5, figsize=(20, 15))  # 4 rows x 5 columns for 20 stocks\n",
    "\n",
    "# for i, ax in enumerate(axes.flatten()):\n",
    "#     ax.plot(predictions[i], label=f'Stock {i+1}')\n",
    "#     ax.set_title(f'Stock {i+1} - 100-Day Prediction')\n",
    "#     ax.set_xlabel('Days')\n",
    "#     ax.set_ylabel('Price')\n",
    "#     ax.grid(True)\n",
    "#     ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "def inverse_scale_timeseries_data(scaled_data, mean, std):\n",
    "    # Reverse the standardization process\n",
    "    return scaled_data * std + mean\n",
    "\n",
    "predictions_original_scale = inverse_scale_timeseries_data(prediction_diff, mean.unsqueeze(-1), std.unsqueeze(-1))\n",
    "\n",
    "\n",
    "plot_timeseries(predictions_original_scale[:,:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
